Question,A,B,C,D,E,Answer
Which of the following methods/techniques is used to identify correlation between data points?,Boxplot,Scatter Plot,Histogram,Bar Graph,Pie Graph,B
What operation is used to combine data from two different tables?,Select,Group,Summarize,Filter,Join,E
Data cleaning means,Removing missing values,Identifying a research question,Model evaluation,Identifying missing values,Removing duplicate values,"A,D,E"
"For a random experiment, all possible outcomes are called",Numerical space,Event space,Sample space,Both B and C,(Empty),D
The covariance is,A measure of the strength of relationship between two variables,Covariance zero indicates that two random variables are independent,Dependent on the units of measurement of the variables,Two independent random variables have covariance of zero,(Empty),"A,D"
What does the term outlier mean?,An extreme value at either end of the distribution,A score that is left out of the analysis because of missing data,The arithmetic mean,A type of variable that cannot be quantified,(Empty),A
Hypothesis terminology in machine learning is defined as:,A function that maps the input space to the output space,A vector of parameters that controls model behaviour,Proposed explaination for a phenomenon,None of the above,(Empty),"A,B"
Which of the following statements are true,Training error decreases by increasing the number of features,Training error increases by increases the number of features,"When the feature space is larger, overfitting is more likely",Decreasing training error and increasing testing error indicates underfitting,(Empty),"A,C"
Generalization error is:,The performance of the model on a validation set,The performance of the model on a training set,Computed from a dataset,Error on unseen data,(Empty),"A,D"
Choose the wrong statements:,"For k-fold cross validation, larger values of k imply less bias","For k-fold cross validation, larger values of k imply more bias","For k-fold cross validation, larger values of k imply less variance","For k-fold cross validation, larger values of k imply more variance",(Empty),"A,D"
Choose the correct statements:,A learning rate is a hyperparameter in gradient descent to update the parameters of the model,Gradient descent is a supervised learning technique,"If the learning rate is too small, then gradient descent may take a very long time to converge","Setting the learning rate to be very small is not harmful, and can only speed up the convergence of gradient descent",(Empty),"A,C"
What do we mean by hard margin?,The SVM model allows a very low amount of error in the classification,The SVM model allows a very high amount of error in the classification,Estimating class probabilities,Separating classes with non linear decision boundary,(Empty),A
Which of the following techniques are used to find the best decision boundary in logistic regression?,Least square error,Maximum likelihood,Mean squared error,Jaccard Distance,(Empty),B
Which statement is true about the slack variable,Slack variable is defined for eah one of the data points,Slack variable allows adding penalty to the objective function,When slack variable is equal to zero then the point lies between the margin,When the slack variable is equal to zero then the point lies on the wrong side of the decision bounary,(Empty),"A,B"
Select the correct answers,Selection of kernel is very important in effectiveness of SVM,Kernel parameters are very important in effectiveness of SVM,Soft margin parameter is very important in effectiveness of SVM,None of the above,(Empty),"A,B,C"
Which of the following can be used when training data is not linearly separable?,Soft margin,Hard margin,Linear logistic regression,Kernel trick,(Empty),"A,D"
Which of the following statements are true about random forests and decision trees?,Both can be used for classification and regression tasks,Random forests can be used only for classification,Decision trees can only be used for regression,Random forests can only be used for regression,(Empty),A
Which of the following statements is true about k-nearest neighbor in terms of bias,Increasing the number of k increases the bias,Decreasing the number of k increases the bias,Cannot say,None of the above,(Empty),A
Building a KNN classifier results in 100% accuracy on training data but the model performs poorly on testing set. Which of the following has gone wrong?,Overfitting,Underfitting,Cannot say,None of the above,(Empty),A
What is the best value of K in k-nearest neighbour?,10% of the size of the dataset,using k-fold cross validation to select the best k,Higher values for K is best,Lower values for k is best,(Empty),B
What are hyperparameters of a neural network?,Batch size,Learning rate,L1 and L2 regularization,Drop out,(Empty),"A,B,C,D"
Non linearity is added to neural network models by:,Stochastic gradient descent,Rectified linear unit,Quadratic programming,Hyperbolic tangent,(Empty),"B,D"
What techniques can be used for bias and variance trading off in deep learning,Drop out,Regularization,Back propagation,Early stopping,(Empty),"A,B,D"
Select possible loss functions for neural networks:,Hinge loss,Cross Entropy loss,Mean squared error,Root mean squared error,(Empty),"B,C,D"
Bagging is:,Training different types of models on randomly chosen subsets of the data,Training different models on the data in sequence,Training the same model multiple times,Not enough information to decide,(Empty),A
Boosting is:,Training different types of models on randomly chosen subsets of the data,Training different models on the data in sequence,Training the same model multiple times,Not enough information to decide,(Empty),B
Ensemble models can make predictions based off of:,Majority vote (ie. Each model gets 1 vote),Weighted average prediction,(Empty),(Empty),(Empty),"A,B"
The Gini impurity is:,"The information (or ""surprise"") of a random variable",How often a randomly chosen element from the set would be incorrectly labeled if it was labeled by the distribution of samples in that node,A measurement of information gain,Used as a heuristic for solving an NP-Complete problem,(Empty),"B,C,D"
Entropy is:,"The information (or ""surprise"") of a random variable",How often a randomly chosen element from the set would be incorrectly labeled if it was labeled by the distribution of samples in that node,A measurement of information gain,Used as a heuristic for solving an NP-Complete problem,(Empty),"A,D"
"Suppose, we have a binary classification problem with 3 input features and we apply a bagging algorithm on this data. Suppose each one of our estimators have 60% accuracy. What can we tell about the maximum accuracy that we can gain by using a bagging algorithm?",Less than 60%,Equal to 60%,More than 60%,We cannot guess without seeing the data,(Empty),C
"In random forest algorithm, we generate hundreds of trees and then aggregate the results of these trees. Consider the individual trees in building a random forest classifier. Please specify the statements that are true about these individual trees.",Individual trees select a subset of the features,Individual trees select all the available features,Individual trees select a subset of observations,Individual trees select all the observations,(Empty),"A,C"
A parametric algorithm:,Is a learning model that summarizes data with a set of parameters of fixed size,Is a learning model which seeks to best fit the training data by constructing the mapping function itself,Is generally faster than nonparametric algorithms,Requires more data than nonparametric algorithms,(Empty),"A,C"
A nonparametric algorithm:,Is a learning model that summarizes data with a set of parameters of fixed size,Is a learning model which seeks to best fit the training data by constructing the mapping function itself,Is generally faster than parametric algorithms,Requires more data than parametric algorithms,(Empty),"B,D"
A model hyperparameter is:,Internal to the model and can be estimated from the data,External to the model and cannot be estimated from the data,Are often manually set,Are often set using heuristics,(Empty),"B,C,D"
Some different methods of feature scaling include:,Min-Max Scaling,Standard Scaling,Normalized Scaling,Mean Scaling,(Empty),"A,B"
Min-Max Scaling is:,[x-min(x)]/[max(x)-min(x)],(x-x̄)/σ,(Empty),(Empty),(Empty),A
Standard Scaling is:,[x-min(x)]/[max(x)-min(x)],(x-x̄)/σ,(Empty),(Empty),(Empty),B
