Question,A,B,C,D,E,Answer
Which of the following methods/techniques is used to identify correlation between data points?,Boxplot,Scatter Plot,Histogram,Bar Graph,Pie Graph,B
What operation is used to combine data from two different tables?,Select,Group,Summarize,Filter,Join,E
Data cleaning means,Removing missing values,Identifying a research question,Model evaluation,Identifying missing values,Removing duplicate values,"A,D,E"
"For a random experiment, all possible outcomes are called",Numerical space,Event space,Sample space,Both B and C,(Empty),D
The covariance is,A measure of the strength of relationship between two variables,Covariance zero indicates that two random variables are independent,Dependent on the units of measurement of the variables,Two independent random variables have covariance of zero,(Empty),"A,D"
What does the term outlier mean?,An extreme value at either end of the distribution,A score that is left out of the analysis because of missing data,The arithmetic mean,A type of variable that cannot be quantified,(Empty),A
Hypothesis terminology in machine learning is defined as:,A function that maps the input space to the output space,A vector of parameters that controls model behaviour,Proposed explaination for a phenomenon,None of the above,(Empty),"A,B"
Which of the following statements are true,Training error decreases by increasing the number of features,Training error increases by increases the number of features,"When the feature space is larger, overfitting is more likely",Decreasing training error and increasing testing error indicates underfitting,(Empty),"A,C"
Generalization error is:,The performance of the model on a validation set,The performance of the model on a training set,Computed from a dataset,Error on unseen data,(Empty),"A,D"
Choose the wrong statements:,"For k-fold cross validation, larger values of k imply less bias","For k-fold cross validation, larger values of k imply more bias","For k-fold cross validation, larger values of k imply less variance","For k-fold cross validation, larger values of k imply more variance",(Empty),"A,D"
Choose the correct statements:,A learning rate is a hyperparameter in gradient descent to update the parameters of the model,Gradient descent is a supervised learning technique,"If the learning rate is too small, then gradient descent may take a very long time to converge","Setting the learning rate to be very small is not harmful, and can only speed up the convergence of gradient descent",(Empty),"A,C"
What do we mean by hard margin?,The SVM model allows a very low amount of error in the classification,The SVM model allows a very high amount of error in the classification,Estimating class probabilities,Separating classes with non linear decision boundary,(Empty),A
Which of the following techniques are used to find the best decision boundary in logistic regression?,Least square error,Maximum likelihood,Mean squared error,Jaccard Distance,(Empty),B
Which statement is true about the slack variable,Slack variable is defined for eah one of the data points,Slack variable allows adding penalty to the objective function,When slack variable is equal to zero then the point lies between the margin,When the slack variable is equal to zero then the point lies on the wrong side of the decision bounary,(Empty),"A,B"
Select the correct answers,Selection of kernel is very important in effectiveness of SVM,Kernel parameters are very important in effectiveness of SVM,Soft margin parameter is very important in effectiveness of SVM,None of the above,(Empty),"A,B,C"
Which of the following can be used when training data is not linearly separable?,Soft margin,Hard margin,Linear logistic regression,Kernel trick,(Empty),"A,D"
Which of the following statements are true about random forests and decision trees?,Both can be used for classification and regression tasks,Random forests can be used only for classification,Decision trees can only be used for regression,Random forests can only be used for regression,(Empty),A
Which of the following statements is true about k-nearest neighbor in terms of bias,Increasing the number of k increases the bias,Decreasing the number of k increases the bias,Cannot say,None of the above,(Empty),A
Building a KNN classifier results in 100% accuracy on training data but the model performs poorly on testing set. Which of the following has gone wrong?,Overfitting,Underfitting,Cannot say,None of the above,(Empty),A
What is the best value of K in k-nearest neighbour?,10% of the size of the dataset,using k-fold cross validation to select the best k,Higher values for K is best,Lower values for k is best,(Empty),B
What are hyperparameters of a neural network?,Batch size,Learning rate,L1 and L2 regularization,Drop out,(Empty),"A,B,C,D"
Non linearity is added to neural network models by:,Stochastic gradient descent,Rectified linear unit,Quadratic programming,Hyperbolic tangent,(Empty),"B,D"
What techniques can be used for bias and variance trading off in deep learning,Drop out,Regularization,Back propagation,Early stopping,(Empty),"A,B,D"
Select possible loss functions for neural networks:,Hinge loss,Cross Entropy loss,Mean squared error,Root mean squared error,(Empty),"B,C,D"
